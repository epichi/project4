{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723789af",
   "metadata": {},
   "source": [
    "# Coding in Action Lab II -- Group Project A #\n",
    "\n",
    "You have to complete this project if your group number is in (1,4,7,10,...), i.e., if it is a multiple of 3, plus 1.\n",
    "\n",
    "### Instructions ###\n",
    "\n",
    "- Fill the following cell with the name, student ID and luiss email address of all members of  your team.\n",
    "\n",
    "- Using this notebook, complete the following list of tasks, **motivating** your choices and **commenting** on your findings.\n",
    "\n",
    "- Turn in the notebook with your work on the course page on learn.luiss.it, once for each member of the group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb56447",
   "metadata": {},
   "source": [
    "This team is composed by:\n",
    "\n",
    "(write your name, student ID and email address)\n",
    ":Edoardo Pichi 285291, Vincenzo Zoino 279731, Edoardo Koelliker 288831, Valentina Franci 288291\n",
    "\n",
    "\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3b079",
   "metadata": {},
   "source": [
    "# Project Assignment part #1 -- Due by June 14, 23:59 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ace890",
   "metadata": {},
   "source": [
    "## Complexity analysis Problems ##\n",
    "\n",
    "- **P1:** Explain the working principles of the following sorting algorithm, and discuss its complexity in terms of the length $n$ of the input array\n",
    "- **P2:** Measure the computational complexity of the algorithm experimentally, using the running time of the corrisponding function for inputs of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb5928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortA(array):\n",
    "    n = len(array)\n",
    "    for i in range(n):\n",
    "        already_sorted = True\n",
    "        for j in range(n - i - 1):\n",
    "            if array[j] > array[j + 1]:\n",
    "                array[j], array[j + 1] = array[j + 1], array[j]\n",
    "                already_sorted = False\n",
    "        if already_sorted:\n",
    "            break\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317b460",
   "metadata": {},
   "source": [
    "\n",
    "## Data analysis Tasks ##\n",
    "\n",
    "### Data preparation and overview ###\n",
    "\n",
    "- **T1:** Using pandas, import the dataset included in the 'data/' folder. \n",
    "- **T2:** Check and clean the dataset:\n",
    "    - **T2.1:** Verify whether the dataset contains null/missing values or duplicated rows. Deal with them appropriately.\n",
    "    - **T2.2:** Check whether the dataset contains any useless column that can be dropped.\n",
    "    - **T2.3:** Check the data types assigned to the columns of the dataframe and change them if needed.\n",
    "- **T3:** Overview:\n",
    "    - **T3.1:** Visualize a sample and a summary of the data.\n",
    "    - **T3.2:** Briefly describe the content of the dataset and what each column of the dataframe contains.\n",
    "    - **T3.3:** Plot the statistical distribution of all numeric columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eed553",
   "metadata": {},
   "source": [
    "________\n",
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afe4b9",
   "metadata": {},
   "source": [
    "# Project Assignment part #2 -- Due by June 16, 23:59 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcff4cd",
   "metadata": {},
   "source": [
    "\n",
    "## Data analysis Tasks ##\n",
    "\n",
    "### Data exploration ###\n",
    "\n",
    "- **T4:** Time distribution of sales:\n",
    "    - **T4.1:** Find the dates of the first and last sales in the dataset and plot the distribution of sales over time.\n",
    "    - **T4.2:** Analyze sales amounts by quarter, month, week, and day.\n",
    "- **T5:** Geographic distribution of sales:\n",
    "    - **T5.1:** Find the city with the greatest number of sales and the greatest volume (total value) of sales.\n",
    "    - **T5.3:** Plot the distribution of the number of sales per city.\n",
    "\n",
    "### Product analysis ###\n",
    "\n",
    "- **T6:** Top sellers:\n",
    "    - **T6.1:** Find the number of units sold for each item and plot the distribution of these numbers.\n",
    "    - **T6.2:** Find the 10 top selling products by number of units sold.\n",
    "    - **T6.3:** Find the 10 top selling products by total value of sales.\n",
    "    - **T6.4:** Group the sales by item type and plot the distribution of the number of units sold per type.\n",
    "    - **T6.5:** Verify and discuss the presence of outliers in the distributions obtained in T6.1 and T6.4.\n",
    "- **T7:** Price-sales relation:\n",
    "    - **T7.1:** Verify that each product has always been sold at the same price.\n",
    "    - **T7.2:** Using suitable plots and metrics, discuss how the number of items sold relates to the item price.\n",
    "    - **T7.3:** For each product type, find the average price and the total number of sales for items of that type.\n",
    "    - **T7.4:** Using suitable plots and metrics, show how the total number of sales of a item type relates to the average price for items of that type.\n",
    "    \n",
    "### Customer analysis ###\n",
    "- **T8:** User similarity:\n",
    "    - **T8.1:** Associate a vector to each customer based on how many items of each type that customer bought.\n",
    "    - **T8.2:** Associate a vector to each customer based on how much that customer spent, in total, for each item type.\n",
    "    - **T8.3:** Compute the similarity between different customers, first using the vectors of T8.1, then the vectors of T8.2; compare and discuss the results.\n",
    "    \n",
    "\n",
    "Remark: if the size of your dataset makes it impossible to complete the analysis in time, run the analysis on a random sample of the original dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48bd128",
   "metadata": {},
   "source": [
    "_______\n",
    "_______\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e32e7e",
   "metadata": {},
   "source": [
    "# Project Assignment part #3 -- Due by June 20, 23:59 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fc7f5",
   "metadata": {},
   "source": [
    "\n",
    "## Data analysis Tasks ##\n",
    "\n",
    "###  Customer analysis ###\n",
    "\n",
    "- **T9:** Customer exploratory analysis:\n",
    "    - **T9.1:** Group the sales by customer and, for each customer, find the number of different purchased items, the total number of purchased items, the total number of orders, and the average amount spent per order.\n",
    "    - **T9.2:** Plot the statistical distribution of those four quantities.\n",
    "- **T10:** Recency-Frequency-Monetary (RFM) model:\n",
    "    - **T10.1:** Build a RFM dataframe.\n",
    "    - **T10.2:** Standardize each column, verify the existence of outliers and treat them appropriately.\n",
    "    - **T10.3:** Use the k-means method to classify the customers, using the elbow method to choose k.\n",
    "    - **T10.4:** Label each cluster and use a histogram to visualize the number of customers in each cluster.\n",
    "\n",
    "### Similarity and recommender systems ###\n",
    "\n",
    "- **T11:** User-based collaborative filtering:\n",
    "    - **T11.1:** For each user, define a user vector as long as the number of unique products in the dataset and whose $i$-th element is 1 if the user has bouth product $i$, 0 otherwise.\n",
    "    - **T11.2:** Use the vectors defined in T11.1 to build a user similarity matrix.\n",
    "    - **T11.3:** Use the similarity matrix defined in T11.2 to define a function that, given a user, creates recommendations for that user.\n",
    "- **T12:** Product similarity:\n",
    "    - **T12.1:** For each product, define a product vector as long as the number of unique users in the dataset and whose $i$-th element is 1 if the product was bouth by user $i$, 0 otherwise.\n",
    "    - **T12.2:** Use SVD to extract a low-dimensional representation for the products.\n",
    "    - **T12.3:** Select a random sample of 10 products and find the 5 products that are most similar to each of those 10 products, using the low-dimensional vectors found in T12.2.\n",
    "    \n",
    "Remark: if the size of your dataset makes it impossible to complete the analysis in time, run the analysis on a random sample of the original dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441ecf4",
   "metadata": {},
   "source": [
    "_______\n",
    "_______\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51e8ce",
   "metadata": {},
   "source": [
    "# Project Assignment part #4 -- Due by June 22, 23:59 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a06da",
   "metadata": {},
   "source": [
    "\n",
    "## Data analysis Tasks ##\n",
    "\n",
    "### Machine Learning ###\n",
    "\n",
    "- **T13:** Regression:\n",
    "    - **T13.1:** Import the dataset included in the 'new_data' folder and drop the date. Standardize the dataset, select the feature that better correlates with the bike rental count and fit a linear regression.\n",
    "    - **T13.2:** Using K-fold cross validation, train a Random Forest regressor to predict the bike rental count.\n",
    "    - **T13.3:** Evaluate the quality of your predictions.\n",
    "- **T14:** Binary classification:\n",
    "    - **T14.1:** Divide the records into 2 classes based on the bike rental count.\n",
    "    - **T14.2:** Using K-fold cross validation, train a Random Forest binary classifier to predict to which of the 2 classes a record belongs.\n",
    "    - **T14.3:** Evaluate the quality of your classifier.\n",
    "\n",
    "Remark: if the size of your dataset makes it impossible to complete the analysis in time, run the analysis on a random sample of the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a01d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected feature is: temperature\n",
      "The Mean Squared Error of the model is: 0.6770406326532351\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('bike_rental.csv')\n",
    "\n",
    "\n",
    "df = df.drop('date', axis=1)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "\n",
    "correlations = df_scaled.corr()['bike_rental_count'].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "feature = correlations.index[1]\n",
    "\n",
    "\n",
    "X = df_scaled[[feature]]\n",
    "y = df_scaled['bike_rental_count']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = reg.predict(X_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "print(f'The selected feature is: {feature}')\n",
    "print(f'The Mean Squared Error of the model is: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdcbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment\n",
    "#This script does the following:\n",
    "\n",
    "#Imports the .csv file.\n",
    "#Drops the 'date' column.\n",
    "#Standardizes the dataset using StandardScaler.\n",
    "#Performs correlation analysis and selects the feature that has the highest correlation with 'bike_rental_count'.\n",
    "#Splits the dataset into a training set and a test set.\n",
    "#Trains a linear regression model on the training data.\n",
    "#Makes predictions using the test data.\n",
    "#Calculates and prints the mean squared error of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d9f6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for each fold: [0.04032597 0.10212621 0.18199063 0.13168742 0.09737554]\n",
      "Mean MSE: 0.11070115538751513\n",
      "Standard deviation of MSE: 0.046317125181229823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "X = df_scaled.drop('bike_rental_count', axis=1)\n",
    "y = df_scaled['bike_rental_count']\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "mse_scores = -scores\n",
    "\n",
    "\n",
    "print('MSE for each fold:', mse_scores)\n",
    "\n",
    "\n",
    "print('Mean MSE:', np.mean(mse_scores))\n",
    "print('Standard deviation of MSE:', np.std(mse_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4064cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment\n",
    "#In this script:\n",
    "\n",
    "#We define X and y for our model. We're using all columns (except 'bike_rental_count') as our features.\n",
    "#We initialize a Random Forest Regressor.\n",
    "#We use the cross_val_score function to perform 5-fold cross validation. This function automatically splits the data into folds, trains the model, makes predictions, and calculates the negative mean squared error (neg_mean_squared_error).\n",
    "#We multiply the scores by -1 to convert them back into mean squared error values.\n",
    "#We print the MSE for each fold, as well as the mean and standard deviation of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c76c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.05338009516121147\n",
      "MAE: 0.14795718260852114\n",
      "R^2: 0.9456433321312617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R^2: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ec86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment\n",
    "#The mean_squared_error and mean_absolute_error functions calculate the MSE and MAE respectively, which measure the average magnitude of the errors in a set of predictions, without considering their direction.\n",
    "\n",
    "#The r2_score function computes the coefficient of determination, also known as R-squared, which provides a measure of how well the model's predictions fit the actual values. R-squared values range from 0 to 1, where 1 indicates a perfect fit, and 0 indicates that the model fails to predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a9d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     season  year     month      hour   holiday   weekday  workingday  \\\n",
      "0 -1.114016   0.0 -1.514656 -1.679897 -0.160782  1.494601   -1.473958   \n",
      "1 -1.114016   0.0 -1.514656 -1.535133 -0.160782  1.494601   -1.473958   \n",
      "2 -1.114016   0.0 -1.514656 -1.390369 -0.160782  1.494601   -1.473958   \n",
      "3 -1.114016   0.0 -1.514656 -1.245605 -0.160782  1.494601   -1.473958   \n",
      "4 -1.114016   0.0 -1.514656 -1.100840 -0.160782  1.494601   -1.473958   \n",
      "\n",
      "    weather  temperature  humidity  wind_speed  bike_rental_count rental_class  \n",
      "0 -0.675501    -1.053087  0.959465   -1.664262          -0.893100          low  \n",
      "1 -0.675501    -1.147375  0.910326   -1.664262          -0.707487          low  \n",
      "2 -0.675501    -1.147375  0.910326   -1.664262          -0.769358          low  \n",
      "3 -0.675501    -1.053087  0.664629   -1.664262          -0.916302          low  \n",
      "4 -0.675501    -1.053087  0.664629   -1.664262          -1.009109          low  \n",
      "rental_class\n",
      "low     2396\n",
      "high    2366\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv('bike_rental.csv')\n",
    "\n",
    "\n",
    "df = df.drop('date', axis=1)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "\n",
    "median_bike_rental_count = df_scaled['bike_rental_count'].median()\n",
    "\n",
    "\n",
    "df_scaled['rental_class'] = df_scaled['bike_rental_count'].apply(lambda x: 'high' if x > median_bike_rental_count else 'low')\n",
    "\n",
    "\n",
    "print(df_scaled.head())\n",
    "\n",
    "\n",
    "print(df_scaled['rental_class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be0f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [0.52465897 0.44071354 0.67121849 0.73529412 0.57668067]\n",
      "Mean accuracy: 0.5897131570361618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "X = df_scaled.drop(['bike_rental_count', 'rental_class'], axis=1)\n",
    "y = df_scaled['rental_class']\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "scores = cross_val_score(rfc, X, y, cv=5)\n",
    "\n",
    "\n",
    "print('Accuracy for each fold:', scores)\n",
    "\n",
    "\n",
    "print('Mean accuracy:', scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03a2d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment\n",
    "\n",
    "\n",
    "#In this script:\n",
    "\n",
    "#We define X and y for our model. We're using all columns (except 'bike_rental_count' and 'rental_class') as our features.\n",
    "#We initialize a Random Forest Classifier.\n",
    "#We use the cross_val_score function to perform 5-fold cross validation. This function automatically splits the data into folds, trains the model, makes predictions, and calculates the accuracy.\n",
    "#We print the accuracy for each fold, as well as the mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f53b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9317943336831059\n",
      "Precision: 0.9315352697095436\n",
      "Recall: 0.9334719334719335\n",
      "F1 Score: 0.932502596053998\n",
      "Confusion Matrix:\n",
      "[[449  32]\n",
      " [ 33 439]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, pos_label='high')\n",
    "\n",
    "\n",
    "recall = recall_score(y_test, y_pred, pos_label='high')\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, pos_label='high')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred, labels=['high', 'low']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cdc0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment\n",
    "#In this script:\n",
    "\n",
    "#accuracy_score computes the accuracy of the model.\n",
    "#precision_score computes the precision of the model. The argument pos_label='high' means we are treating 'high' as the positive class.\n",
    "#recall_score computes the recall of the model.\n",
    "#f1_score computes the F1 score of the model.\n",
    "#confusion_matrix generates the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ae970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
